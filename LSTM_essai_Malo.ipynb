{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1727eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Malo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "import nltk\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "#import panda as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "# Library for boxplots\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#from tensorflow.keras.layers import Attention\n",
    "#from attention import AttentionLayer\n",
    "\n",
    "#from keras_self_attention import SeqSelfAttention\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63486b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path,filename):\n",
    "    json_obj_list=[]\n",
    "    with open(os.path.join(path,filename),'r') as fin:\n",
    "        for row in fin:\n",
    "            json_obj_list.append(json.loads(row))\n",
    "    return json_obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENCIES = {\n",
    "    \"$\": \"USD\", \"zł\": \"PLN\", \"£\": \"GBP\", \"¥\": \"JPY\", \"฿\": \"THB\", \"₡\": \"CRC\", \"₦\": \"NGN\",\"₩\": \"KRW\",\n",
    "    \"₪\": \"ILS\", \"₫\": \"VND\", \"€\": \"EUR\", \"₱\": \"PHP\", \"₲\": \"PYG\", \"₴\": \"UAH\", \"₹\": \"INR\",}\n",
    "CURRENCY_REGEX = re.compile(\n",
    "    \"({})+\".format(\"|\".join(re.escape(c) for c in CURRENCIES.keys())))\n",
    "\n",
    "EMAIL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}(?:$|(?=\\b))\",\n",
    "    flags=re.IGNORECASE | re.UNICODE,)\n",
    "\n",
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions =          {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\", \"i've\": \"i have\"}\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                print(\"contract\")\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "        \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = EMAIL_REGEX.sub(' ',text)\n",
    "    text = CURRENCY_REGEX.sub(' ',text)\n",
    "    text = ' '.join([contractions[t] if t in contractions else t for t in text.split(\" \")])    \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r\"'s\\b\",\"\", text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A remplacer par \"train\"  \"val\"   \"test\"\n",
    "kind_data=\"train\"\n",
    "\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file][:10]\n",
    "diff_abs,diff_des=[],[]\n",
    "for file_name in tqdm(file_names):\n",
    "    \n",
    "    listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name)\n",
    "    #print(listJSON)\n",
    "    #JSONlist=filterChars(os.path.join(\"data\",kind_data,\"g\"),file_name,listJSON)\n",
    "    for i in range(len(listJSON)) :\n",
    "        abstract = listJSON[i]['abstract']\n",
    "        description = listJSON[i]['description']\n",
    "        txt2_ab=clean_text(abstract, remove_stopwords = True)\n",
    "        if len(abstract)!=len(txt2_ab):\n",
    "            print(file_name, \", row \",i,\" abs-abs2 : \",len(abstract)-len(txt2_ab))\n",
    "        diff_abs.append(len(abstract)-len(txt2_ab))\n",
    "        txt2_de=clean_text(description, remove_stopwords = True)\n",
    "        if len(description)!=len(txt2_de):\n",
    "            print(file_name, \", row \",i,\" des-des2 : \",len(description)-len(txt2_de))\n",
    "        diff_des.append(len(description)-len(txt2_de))\n",
    "\n",
    "diff_abs,diff_des=np.array(diff_abs),np.array(diff_des)\n",
    "        \n",
    "sns.boxplot(data=diff_abs,fliersize=10)   \n",
    "plt.show()      \n",
    "sns.boxplot(data=diff_des,fliersize=10)   \n",
    "plt.show()      \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4567c",
   "metadata": {},
   "source": [
    "### Mesure des tailles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e43f0cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_data000000000000.txt\n",
      "new_data000000000001.txt\n",
      "new_data000000000002.txt\n",
      "new_data000000000003.txt\n",
      "new_data000000000004.txt\n",
      "new_data000000000005.txt\n",
      "new_data000000000006.txt\n",
      "new_data000000000007.txt\n",
      "new_data000000000008.txt\n",
      "new_data000000000009.txt\n",
      "new_data000000000010.txt\n",
      "new_data000000000011.txt\n",
      "new_data000000000012.txt\n",
      "new_data000000000013.txt\n",
      "new_data000000000014.txt\n",
      "new_data000000000015.txt\n",
      "new_data000000000016.txt\n",
      "new_data000000000017.txt\n",
      "new_data000000000019.txt\n",
      "new_data000000000020.txt\n",
      "new_data000000000021.txt\n",
      "new_data000000000022.txt\n",
      "new_data000000000023.txt\n",
      "new_data000000000024.txt\n",
      "new_data000000000025.txt\n",
      "new_data000000000026.txt\n",
      "new_data000000000027.txt\n",
      "new_data000000000028.txt\n",
      "new_data000000000029.txt\n",
      "new_data000000000030.txt\n",
      "new_data000000000033.txt\n",
      "new_data000000000034.txt\n",
      "new_data000000000035.txt\n",
      "new_data000000000036.txt\n",
      "new_data000000000037.txt\n",
      "new_data000000000038.txt\n",
      "new_data000000000039.txt\n",
      "new_data000000000040.txt\n",
      "new_data000000000041.txt\n",
      "new_data000000000042.txt\n",
      "new_data000000000043.txt\n",
      "new_data000000000044.txt\n",
      "new_data000000000045.txt\n",
      "new_data000000000046.txt\n",
      "new_data000000000047.txt\n",
      "new_data000000000048.txt\n",
      "new_data000000000049.txt\n",
      "new_data000000000050.txt\n",
      "new_data000000000051.txt\n",
      "new_data000000000052.txt\n",
      "new_data000000000053.txt\n",
      "new_data000000000054.txt\n",
      "new_data000000000055.txt\n",
      "new_data000000000056.txt\n",
      "new_data000000000057.txt\n",
      "new_data000000000058.txt\n",
      "new_data000000000060.txt\n",
      "new_data000000000061.txt\n",
      "new_data000000000062.txt\n",
      "new_data000000000063.txt\n",
      "new_data000000000064.txt\n",
      "new_data000000000065.txt\n",
      "new_data000000000066.txt\n",
      "new_data000000000067.txt\n",
      "new_data000000000068.txt\n",
      "new_data000000000069.txt\n",
      "new_data000000000070.txt\n",
      "new_data000000000071.txt\n",
      "new_data000000000072.txt\n",
      "new_data000000000073.txt\n",
      "new_data000000000074.txt\n",
      "new_data000000000075.txt\n",
      "new_data000000000076.txt\n",
      "new_data000000000077.txt\n",
      "new_data000000000078.txt\n",
      "new_data000000000079.txt\n",
      "new_data000000000080.txt\n",
      "new_data000000000081.txt\n",
      "new_data000000000082.txt\n",
      "new_data000000000083.txt\n",
      "new_data000000000084.txt\n",
      "new_data000000000085.txt\n",
      "new_data000000000086.txt\n",
      "new_data000000000087.txt\n",
      "new_data000000000088.txt\n",
      "new_data000000000090.txt\n",
      "new_data000000000091.txt\n",
      "new_data000000000092.txt\n",
      "new_data000000000093.txt\n",
      "new_data000000000094.txt\n",
      "new_data000000000095.txt\n",
      "new_data000000000096.txt\n",
      "new_data000000000097.txt\n",
      "new_data000000000098.txt\n",
      "new_data000000000099.txt\n",
      "new_data000000000100.txt\n",
      "new_data000000000101.txt\n",
      "new_data000000000102.txt\n",
      "new_data000000000103.txt\n",
      "new_data000000000105.txt\n",
      "new_data000000000106.txt\n",
      "new_data000000000107.txt\n",
      "new_data000000000108.txt\n",
      "new_data000000000109.txt\n",
      "new_data000000000110.txt\n",
      "new_data000000000111.txt\n",
      "new_data000000000113.txt\n",
      "new_data000000000114.txt\n",
      "new_data000000000115.txt\n",
      "new_data000000000116.txt\n",
      "new_data000000000117.txt\n",
      "new_data000000000118.txt\n",
      "new_data000000000119.txt\n",
      "new_data000000000120.txt\n",
      "new_data000000000121.txt\n",
      "new_data000000000122.txt\n",
      "new_data000000000123.txt\n",
      "new_data000000000124.txt\n",
      "new_data000000000125.txt\n",
      "new_data000000000126.txt\n",
      "new_data000000000127.txt\n",
      "new_data000000000128.txt\n",
      "new_data000000000129.txt\n",
      "new_data000000000130.txt\n",
      "new_data000000000131.txt\n",
      "new_data000000000133.txt\n",
      "new_data000000000134.txt\n",
      "new_data000000000135.txt\n",
      "new_data000000000136.txt\n",
      "new_data000000000137.txt\n",
      "new_data000000000138.txt\n",
      "new_data000000000139.txt\n",
      "new_data000000000141.txt\n",
      "new_data000000000142.txt\n",
      "new_data000000000143.txt\n",
      "new_data000000000144.txt\n",
      "new_data000000000145.txt\n",
      "new_data000000000146.txt\n",
      "new_data000000000147.txt\n",
      "new_data000000000148.txt\n",
      "new_data000000000150.txt\n",
      "new_data000000000151.txt\n",
      "new_data000000000152.txt\n",
      "new_data000000000153.txt\n",
      "new_data000000000154.txt\n",
      "new_data000000000155.txt\n",
      "new_data000000000157.txt\n",
      "new_data000000000158.txt\n",
      "new_data000000000159.txt\n",
      "new_data000000000160.txt\n",
      "new_data000000000161.txt\n",
      "new_data000000000163.txt\n",
      "new_data000000000164.txt\n",
      "new_data000000000165.txt\n",
      "new_data000000000166.txt\n"
     ]
    }
   ],
   "source": [
    "abstract_word_count = []\n",
    "description_word_count = []\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file]\n",
    "for file_name in tqdm(file_names): \n",
    "\n",
    "    listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name)\n",
    "    for i in range(len(listJSON)) :\n",
    "        abstract = listJSON[i]['abstract']\n",
    "        description = listJSON[i]['description']\n",
    "        \n",
    "        abstract_word_count.append(len(abstract.split()))\n",
    "        description_word_count.append(len(description.split()))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "540fdc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "length_df = pd.DataFrame({'descrition': description_word_count, 'abstract': abstract_word_count})\n",
    "length_df.hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fa4a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37456\n",
      "2074\n"
     ]
    }
   ],
   "source": [
    "print(max(description_word_count))\n",
    "print(max(abstract_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b52b96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:56<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "abstract_count = 0\n",
    "description_count = 0\n",
    "count=0\n",
    "lim_abs =160\n",
    "lim_des = 9000\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file]\n",
    "for file_name in tqdm(file_names): \n",
    "\n",
    "    listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name)\n",
    "    for i in range(len(listJSON)) :\n",
    "        count+=1\n",
    "        abstract = listJSON[i]['abstract']\n",
    "        description = listJSON[i]['description']\n",
    "        \n",
    "        if(len(abstract.split())<=lim_abs):\n",
    "            abstract_count += 1\n",
    "        if(len(description.split())<=lim_des):\n",
    "            description_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2151433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nombre de description de taille inférieure à  9000  :  0.99562438449804\n",
      " nombre d 'abstract de taille inférieure à  160  :  0.9963813312221214\n",
      "258935\n"
     ]
    }
   ],
   "source": [
    "print(\" nombre de description de taille inférieure à \",lim_des , \" : \",description_count/count)    \n",
    "print(\" nombre d 'abstract de taille inférieure à \",lim_abs , \" : \",abstract_count/count)   \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08c3c7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcc1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_text = np.array(cleaned_text)\n",
    "cleaned_headlines = np.array(cleaned_headlines)\n",
    "\n",
    "short_text=[]\n",
    "short_headlines=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    \n",
    "    if(len(cleaned_headlines[i].split())<=max_headlines_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_headlines.append(cleaned_headlines[i])\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184b581",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6411b6f",
   "metadata": {},
   "source": [
    "## Sur Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c164c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [14:20<00:00,  5.55s/it]\n"
     ]
    }
   ],
   "source": [
    "kind_data=\"train\"\n",
    "\n",
    "x_tokenizer = Tokenizer() \n",
    "#x_tokenizer.fit_on_texts(_____)\n",
    "\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file]\n",
    "for i in tqdm(range(len(file_names))) : \n",
    "    file_name=file_names[i]\n",
    "    listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name)\n",
    "\n",
    "    descriptions = [i['description'] for i in listJSON  ]#listJSON[:]['description']\n",
    "    x_tokenizer.fit_on_texts(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cae658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 158561/588085 [00:00<00:00, 777620.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referring 367900\n",
      "fig 5199387\n",
      "service 309448\n",
      "technician 13298\n",
      "visiting 2887\n",
      "customer 210628\n",
      "location 405434\n",
      "provided 808434\n",
      "input 1283124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588085/588085 [00:00<00:00, 702400.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 59.7163675319044 %\n",
      "Total Coverage of rare words: 0.11371321630587963 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in tqdm(x_tokenizer.word_counts.items()):\n",
    "    tot_cnt=tot_cnt+1\n",
    "    \n",
    "    tot_freq=tot_freq+value\n",
    "    if tot_cnt<10 :\n",
    "        print(key,value)\n",
    "    \n",
    "    \n",
    "    if(value<thresh):\n",
    "        \n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100,\"%\")\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e695fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [09:13<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "#prepare a LIMITED tokenizer for reviews on training data\n",
    "tokenizer_description = Tokenizer(num_words=tot_cnt-cnt) \n",
    "\n",
    "\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file]\n",
    "for i in tqdm(range(len(file_names))) : \n",
    "    file_name=file_names[i]\n",
    "    listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name)\n",
    "\n",
    "    descriptions = [i['description'] for i in listJSON  ]\n",
    "    tokenizer_description.fit_on_texts(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8574f",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61dd2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [01:00<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer = Tokenizer() \n",
    "#x_tokenizer.fit_on_texts(_____)\n",
    "\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file]\n",
    "for i in tqdm(range(len(file_names))) : \n",
    "    file_name=file_names[i]\n",
    "    listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name)\n",
    "\n",
    "    abstracts = [i['abstract'] for i in listJSON  ]#listJSON[:]['description']\n",
    "    x_tokenizer.fit_on_texts(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9273565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68948/68948 [00:00<00:00, 623812.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 47.82009630446134 %\n",
      "Total Coverage of rare words: 0.2959113934193723 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in tqdm(x_tokenizer.word_counts.items()):\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    \n",
    "    if(value<thresh):\n",
    "        \n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100,\"%\")\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c03e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:56<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#prepare a LIMITED tokenizer for reviews on training label\n",
    "tokenizer_abstract = Tokenizer(num_words=tot_cnt-cnt) \n",
    "\n",
    "\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file]\n",
    "for i in tqdm(range(len(file_names))) : \n",
    "    file_name=file_names[i]\n",
    "    listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name)\n",
    "\n",
    "    abstracts = [i['abstract'] for i in listJSON  ]\n",
    "    tokenizer_abstract.fit_on_texts(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631ab73",
   "metadata": {},
   "source": [
    "## Save des token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b7f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenize_file(path,filename,JSONlist,tokenizer_x,tokenizer_y):\n",
    "    \n",
    "    if os.path.exists(path+'new_tok_'+filename+'.txt')==True:\n",
    "        os.remove(path+'new_tok_'+filename)\n",
    "        \n",
    "    descriptions = [i['description'] for i in JSONlist  ]\n",
    "    x_tr_seq   =  tokenizer_x.texts_to_sequences(descriptions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    abstracts = [i['abstract'] for i in JSONlist  ]\n",
    "    y_tr_seq   =  tokenizer_y.texts_to_sequences(abstracts) \n",
    "    \n",
    "    #print(len(x_tr_seq))\n",
    "    #print(x_tr_seq)\n",
    "    \n",
    "    new_f= open(path+'new_tok'+filename,'a')\n",
    "  \n",
    "    for i in range(len(JSONlist)) :\n",
    "        #JSONlist[i]['abstract']=x_tr_seq[i]\n",
    "        #JSONlist[i]['description']=y_tr_seq[i]\n",
    "    \n",
    "        updatedJSON={\"publication_number\":JSONlist[i]['publication_number'],\"abstract\":x_tr_seq[i],\"description\":y_tr_seq[i]}\n",
    "        new_f.write(json.dumps(updatedJSON))\n",
    "        \n",
    "        \n",
    "    #print(\"Wrote file \"+new_f.name)\n",
    "    new_f.close()\n",
    "    \n",
    "    #f2= open(path+'new_tok'+filename,'r')\n",
    "    #text = f2.read().replace('}{\"publication_number\"','}\\n{\"publication_number\"').encode(\"utf8\")\n",
    "    \n",
    "    #f2.write(str(text))\n",
    "    #f2.close()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b9e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [06:34<00:00,  2.54s/it]\n",
      "100%|██████████| 155/155 [00:23<00:00,  6.50it/s]\n",
      "100%|██████████| 155/155 [00:24<00:00,  6.33it/s]\n"
     ]
    }
   ],
   "source": [
    "#### Enregistrement des données tokénisés\n",
    "os.makedirs(\"data_token\", exist_ok=True)\n",
    "for kind_data in [\"train\",\"val\",\"test\"]:\n",
    "    name_folder0 = \"data_token/\"+kind_data\n",
    "    name_folder = \"data_token/\"+kind_data+\"/g\"\n",
    "    os.makedirs(name_folder0, exist_ok=True)\n",
    "    os.makedirs(name_folder, exist_ok=True)\n",
    "    \n",
    "    file_names = [file for file in os.listdir(os.path.join(\"data\",kind_data,\"g\")) if \".txt\" in file]\n",
    "    for i in tqdm(range(len(file_names))) :\n",
    "        file_name=file_names[i]\n",
    "        listJSON = readData(os.path.join(\"data\",kind_data,\"g\"),file_name) \n",
    "\n",
    "        \n",
    "        \n",
    "        #Padding ?????????????????????????????????????\n",
    "        #x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "        #x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "    \n",
    "        #on save ici le fichier à l'adresse\n",
    "        create_tokenize_file(name_folder+\"/\",file_name,listJSON,tokenizer_description,tokenizer_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a774db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SAVE LES TOKENIZER\n",
    "##MEME TOKENIZER POUR X ET Y ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad2bbeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:03<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3867\n",
      "481\n",
      "[44, 1160, 40074, 1528, 1821, 7, 867, 303, 1078, 9, 151, 170, 900, 52, 21, 26, 441, 565, 31, 31, 25, 35, 167, 35, 82, 31, 2, 84, 1621, 2361, 3589, 88805, 1887, 1438, 1158, 21, 185, 63696, 2855, 174401, 278, 544, 104, 380, 84, 12517, 1530, 12517, 1530, 167, 6, 35, 82, 121, 380, 84, 170, 900, 170, 900, 2155, 82, 31, 25, 35, 82, 1530, 595, 1096, 5254, 900, 1743, 1814, 28, 1466, 132, 489, 6, 51, 31, 25, 35, 167, 88, 1814, 28, 1466, 132, 2584, 278, 1044, 47, 31, 62, 489, 222, 31, 25, 35, 519, 1044, 278, 25, 900, 519, 136, 278, 17, 808, 1466, 132, 123, 2889, 222, 25, 35, 1466, 132, 583, 104, 278, 1044, 47, 31, 2889, 222, 1031, 867, 1160, 2889, 222, 51, 1466, 132, 25, 35, 293, 121, 28, 1466, 132, 28, 121, 1466, 132, 163, 586, 28, 28, 35, 121, 1466, 132, 1466, 132, 1519, 2889, 222, 4, 2490, 5528, 15210, 847, 1577, 2889, 222, 1189, 222, 2, 762, 1088, 1375, 422, 6094, 222, 7278, 1015, 970, 2, 1541, 147, 4941, 31, 544, 104, 278, 166, 501, 7, 1151, 19970, 508, 3727, 128, 41, 31, 334, 314, 111, 163, 586, 28, 28, 35, 2, 15, 586, 80, 31, 112, 163, 586, 28, 121, 1466, 132, 1015, 970, 5545, 544, 678, 63, 80, 11712, 2158, 18, 544, 104, 278, 166, 82, 1530, 121, 1466, 132, 35, 82, 25, 35, 31, 3702, 228, 544, 2, 157, 3578, 82, 117, 41, 3600, 157, 508, 48, 157, 629, 603, 3793, 144, 470, 82, 48, 157, 629, 603, 144, 144, 52, 31, 35, 337, 31, 489, 6, 4169, 62, 508, 128, 31, 2857, 25, 1466, 132, 51, 201, 3578, 82, 351, 489, 6, 31, 3634, 434, 104, 278, 583, 3578, 82, 117, 470, 117, 157, 1407, 508, 48, 157, 629, 603, 3793, 144, 31, 2465, 155, 508, 278, 294, 252, 503, 3578, 117, 41, 6981, 24, 339, 1335, 159, 278, 939, 511, 339, 2306, 31, 20, 1752, 361, 73, 315, 160, 6981, 134, 1208, 339, 544, 277, 461, 278, 2, 205, 2465, 312, 511, 1528, 2410, 2229, 95, 7, 167, 2889, 489, 222, 583, 104, 278, 1044, 47, 31, 1015, 1388, 104, 278, 2792, 60, 199, 51, 1151, 19970, 508, 3727, 128, 31, 41, 1015, 1388, 51, 7426, 544, 31, 80, 11712, 2158, 18, 104, 278, 166, 3578, 82, 117, 508, 31, 128, 2857, 104, 82, 3578, 231, 31, 3634, 906, 104, 278, 3578, 117, 41, 339, 1335, 159, 278, 939, 511, 1444, 670, 4231, 1335, 169, 31, 2992, 418, 990, 2992, 1321, 849, 1528, 1821, 24, 25984, 328, 762, 1088, 35, 82, 334, 314, 111, 1812, 2959, 2204, 8544, 422, 82, 1049, 38, 1053, 340, 2347, 347, 159, 10, 7, 114, 24, 338, 4746, 334, 7, 202, 1278, 96, 289, 2, 19, 89, 384, 487, 537, 139, 760, 745, 234, 7, 334, 314, 111]\n",
      "1649\n",
      "1649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_names = [file for file in os.listdir(os.path.join(\"data\",\"train\",\"g\")) if \".txt\" in file]\n",
    "for i in tqdm(range(len(file_names[0]))) : \n",
    "    file_name=file_names[i]\n",
    "\n",
    "    listJSON = readData(os.path.join(\"data\",\"train\",\"g\"),file_name)\n",
    "    descriptions = [i['description'] for i in listJSON  ]\n",
    "    abstracts = [i['abstract'] for i in listJSON  ]\n",
    "\n",
    "\n",
    "x_tr_seq   =  tokenizer_description.texts_to_sequences(descriptions)\n",
    "\n",
    "print(len(descriptions[0]))\n",
    "print(len(x_tr_seq[0]))\n",
    "print(x_tr_seq[0])\n",
    "\n",
    "print(len(x_tr_seq))\n",
    "print(len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e54be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb843635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54297dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3a915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843d4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c1300",
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(JSONlist)) :\n",
    "        abstract = JSONlist[i]['abstract']\n",
    "        description = JSONlist[i]['description']\n",
    "            \n",
    "        abstract1 = abstract.lower()\n",
    "        description1 = description.lower()\n",
    "            \n",
    "        abstract2 = re.sub('[^a-zA-Z ]',' ',abstract1)\n",
    "        description2 = re.sub('[^a-zA-Z ]',' ',description1)\n",
    "            \n",
    "        abstract3 = re.sub(r'(?:^| )\\w(?:$| )', ' ', abstract2).strip()\n",
    "        description3 = re.sub(r'(?:^| )\\w(?:$| )', ' ', description2).strip()\n",
    "            \n",
    "        abstract4 = ' '.join([word for word in abstract3.split() if word not in cachedstopwords])\n",
    "        description4 = ' '.join([word for word in description3.split() if word not in cachedstopwords])\n",
    "            \n",
    "        abstract5=re.sub(' +',' ',abstract4)\n",
    "        description5=re.sub(' +',' ',description4)\n",
    "            \n",
    "        JSONlist[i]['abstract']=abstract5\n",
    "        JSONlist[i]['description']=description5\n",
    "        updatedJSON={\"publication_number\":JSONlist[i]['publication_number'],\"abstract\":abstract5,\"description\":description5}\n",
    "        new_f.write(json.dumps(updatedJSON))\n",
    "    print(\"Wrote file \"+new_f.name)\n",
    "    new_f.close()\n",
    "    return JSONlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4811a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcaa5976",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-f1867f5fdb64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#prepare a tokenizer for reviews on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_tr' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "\n",
    "print(x_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908f902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
